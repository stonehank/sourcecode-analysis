import path from 'path'
import fs from 'fs'
import url from 'url'
import Server from './Server'
import Crawler from './Crawler'
import Writer from './Writer'
import program from 'safe-commander'

export default () => {
  program
    .version(require('../package.json').version)
    .option('--build-dir <directory>', `Specify where the JS app lives. Defaults to 'build'`)
    .option('--domain <domain>', `The local domain to use for scraping. Defaults to 'localhost'`)
    .option('--output-dir <directory>', `Where to write the snapshots. Defaults to in-place (i.e. same as build-dir)`)
    .parse(process.argv)

  const {
    buildDir = 'build',
    domain = 'localhost',
    outputDir = buildDir,
  } = program.optsObj

  // process.cwd() æ‰§è¡Œnodeå‘½ä»¤æ—¶çš„ç›®å½•(æ­¤å¤„æ˜¯æ ¹ç›®å½•ï¼Œå› ä¸ºåœ¨æ ¹ç›®å½•è°ƒç”¨ npm run build)
  // è¯»å–package.json
  const pkg = JSON.parse(fs.readFileSync(path.join(process.cwd(), 'package.json')))
  // èŽ·å–package.jsonå†…çš„homepage(/) æˆ–è€…ä¸º '/'
  const basename = ((p) => p.endsWith('/') ? p : p + '/')(pkg.homepage ? url.parse(pkg.homepage).pathname : '')

  // èŽ·å–å­˜åœ¨äºŽpackage.jsonä¸­çš„é…ç½®é¡¹ç›®
  const options = Object.assign({
    include: [],
    exclude: [],
    snapshotDelay: 50,
  }, pkg['react-snapshot'] || pkg.reactSnapshot || {})

  // å°†excludeå’Œincludeè·¯å¾„è½¬å˜ä¸ºç»å¯¹è·¯å¾„ å¹¶ä¸”ç”± '\\a\\b'è½¬æ¢æˆ '/a/b'
  options.exclude = options.exclude.map((p) => path.join(basename, p).replace(/\\/g, '/'))
  options.include = options.include.map((p) => path.join(basename, p).replace(/\\/g, '/'))
  // includeå¤´éƒ¨æ’å…¥basename
  options.include.unshift(basename)

  // path.resolveï¼šè§£æžä¸ºç»å¯¹è·¯å¾„
  const buildDirPath = path.resolve(`./${buildDir}`)
  const outputDirPath = path.resolve(`./${outputDir}`)

  // å¦‚æžœä¸å­˜åœ¨buildæ–‡ä»¶å¤¹(æ‰“åŒ…ä¸æˆåŠŸ) æŠ›å‡ºé”™è¯¯
  if (!fs.existsSync(buildDir)) throw new Error(`No build directory exists at: ${buildDirPath}`)

  // å°†åŽŸå§‹çš„index.htmlæ”¹æˆ200.html
  const writer = new Writer(buildDirPath, outputDirPath)
  writer.move('index.html', '200.html')

  const server = new Server(buildDirPath, basename, 0, pkg.proxy)
  // .start() é»˜è®¤ç›‘å¬ä¸€ä¸ªæœªä½¿ç”¨çš„ç«¯å£
  server.start().then(() => {
    const crawler = new Crawler(`http://${domain}:${server.port()}${basename}`, options.snapshotDelay, options)
    return crawler.crawl(({ urlPath, html }) => {
      // æ‰§è¡Œåˆ°æ­¤å¤„ï¼Œå·²ç»åŸºæœ¬å®Œæˆå½“å‰urlPathçš„ç›‘å¬ï¼Œè¿™é‡Œæ˜¯æœ€åŽæ£€æŸ¥æ˜¯å¦ä»¥basenameå¼€å¤´
      // ç›¸å½“äºŽä¼šå°†includesçš„æ‰€æœ‰pathéƒ½æ£€æŸ¥æ˜¯å¦ä»¥basenameå¼€å¤´
      if (!urlPath.startsWith(basename)) {
        console.log(`â— Refusing to crawl ${urlPath} because it is outside of the ${basename} sub-folder`)
        return
      }
      // ä»¥basenameä½œä¸ºæ ¹ç›®å½•
      urlPath = urlPath.replace(basename, '/')
      let filename = urlPath
      // å¦‚æžœæ˜¯ä»¥/ç»“å°¾ï¼Œä¾‹å¦‚ï¼š/b/a/ æ·»åŠ index.htmlï¼Œ /b/a/index.html
      // å¦‚æžœæ— åŽç¼€ï¼Œä¾‹å¦‚ï¼š/b/a æ·»åŠ åŽç¼€.htmlï¼Œ /b/a.html
      if (urlPath.endsWith('/')) {
        filename = `${urlPath}index.html`
      } else if (path.extname(urlPath) == '') {
        filename = `${urlPath}.html`
      }
      console.log(`âœï¸   Saving ${urlPath} as ${filename}`)
      // å†™å…¥
      writer.write(filename, html)
    })

  }).then(() => server.stop(), err => console.log(`ðŸ”¥ ${err}`))
}
